	var wg sync.WaitGroup
	wg.Add(partitions + 1)

	nextPartition := uint32(0)

	go func() {
		wg.Done()
	}()

	thr := runtime.NumCPU()
	for i := 0; i < thr; i++ {
		go func() {
			for {
				p := int(atomic.AddUint32(&nextPartition, 1))-1
				if p >= partitions {
					break
				}
				b,e := partitionRange(p)
				for ;b!=e; b++ {

				}
			}
			wg.Done()
		}()
	}

	wg.Wait()


    type Pyramid struct {
	Len     int
	MinZoom int
	MaxZoom int
	h       []*hilbert.Hilbert // one fore each zoom
	start   []int
}

func (p *Pyramid) Xyz(id int) (x, y, z int, e error) {
	for z, h := range p.h {
		sz := h.N * h.N
		if id >= sz {
			id -= sz
		} else {
			x, y, _ = h.Map(id)
			return x, y, z + p.MinZoom, nil
		}
	}
	return 0, 0, 0, fmt.Errorf("out of bounds")
}
func (p *Pyramid) FromXyz(x, y, z int) (id int, e error) {
	

	return 0, fmt.Errorf("out of bounds")
}

func NewPyramid(minZoom, maxZoom int) *Pyramid {
	o := &Pyramid{
		MinZoom: minZoom,
		MaxZoom: maxZoom,
		h:       []*hilbert.Hilbert{},
	}
	o.h = make([]*hilbert.Hilbert, o.MaxZoom-o.MinZoom)
	o.start = make([]int, 1 + o.MaxZoom-o.MinZoom)
	cnt := 0
	for x := range o.h {
		o.h[x], _ = hilbert.NewHilbert(1 << (x + o.MinZoom))
		cnt += o.h[x].N * o.h[x].N
		o.start[x+1] = cnt

	}
	o.Len = cnt
	// for z := o.MinZoom; z < o.MaxZoom; z++ {
	// 	o.Len += (1 << z) * (1 << z)
	// }
	return o
}
	rs, e := db.Query("select tile_id,count(*) from shallow_tiles group by 1 having count(*)>1 order by 2")
	if e != nil {
		return e
	}
	var tile_id uint32
	symbol_table := map[uint32]uint32{}
	count := uint32(0)
	for rs.Next() {
		rs.Scan(&tile_id)
		symbol_table[tile_id] = count
		count++
	}
	rs.Close()



// the main value of having partitions is allowing the smallest reasonable files
// we could write them sequentially, should we?
// the first partition will always be reused tiles in this scheme
func WriteMbtiles(inpath string, outpath string, partitions int, maxfile int) error {
	thr := runtime.NumCPU()

	db, e := sql.Open("sqlite", inpath)
	if e != nil {
		return e
	}
	getTileData, e := db.Prepare("select * from tiles_data where tile_data_id=?")
	if e != nil {
		panic(e)
	}

	// count the tiles? this should just be the pyramid.
	pyr := NewPyramid(0, 15)
	splits := PartitionRange(0, pyr.Len, partitions)
	// how should partitioning interact with the clipping polygon(s)?

	// get the reused tiles and cluster them to make them easier to cache.
	// we might as well read the entire shallow_tiles table and then sort by hilbert
	// we can do our own binning then.

	// how should we change this to allow a range of x,y,14?
	// this is pyramid, tile so we can sort by hilbert but fetch from database by tile.
	// should we map
	shallow := make([]uint64, pyr.Len)
	// this is count, tile. lets us sort by count so we can pack the most reused tiles to make them easy to cache. Count here _should_ be a weight that includes usefullness but future work.
	useCount := make([]uint64, pyr.Len)
	rs, e := db.Query("select  from zoom_level integer, tile_column integer,tile_row integer, tile_data_id integer")
	if e != nil {
		return e
	}
	var zoom_level, tile_column, tile_row, tile_data_id int
	i := 0
	for rs.Next() {
		rs.Scan(&zoom_level, &tile_column, &tile_row, &tile_data_id)
		// generate the hilbert id and
		id, e := pyr.FromXyz(tile_column, tile_row, zoom_level)
		if e != nil {
			panic(e)
		}
		shallow[i] = (uint64(id) << 32) + uint64(tile_data_id)
		useCount[tile_data_id]++
	}
	// sort the useCount by the count we should do this loop with pargo
	for j := 0; j < i; i++ {
		useCount[j] = (useCount[j] << 32) + uint64(j)
	}
	sorty.SortSlice(useCount)

	// now we can map the reused tiles
	tile_cache := map[uint32]uint32{}

	var repeated int
	for i, v := range useCount {
		count, tile_data_id := unpair(v)
		if count < 2 {
			repeated = i
			break
		}
		tile_cache[uint32(tile_data_id)] = uint32(i)
	}

	// our final data will have reuse partition. how should we reference it?

	var wg sync.WaitGroup
	wg.Add(partitions + 1)
	nextPartition := uint32(0)

	// write the reused partition
	go func() {
		var tile_data []byte
		log.Printf("Writing reused %d", repeated)
		for i := 0; i < 10; i++ {
			log.Printf("")
		}
		wr := OpenSplitLog(fmt.Sprintf("%s_", outpath), maxfile)
		for i := 0; i < repeated; i++ {
			_, tile_data_id := unpair(useCount[i])
			getTileData.QueryRow(tile_data_id).Scan(&tile_data)
			wr.Write(tile_data)
		}

		wg.Done()
	}()

	for i := 0; i < thr; i++ {
		go func() {
			var tile_data []byte
			for {
				p := int(atomic.AddUint32(&nextPartition, 1)) - 1
				if p >= partitions {
					break
				}
				b := splits[p]
				e := splits[p+1]
				log.Printf("Writing %d,%d to %d", b, e, p)
				// write the entire partition
				if !dry_run {
					wr := OpenSplitLog(fmt.Sprintf("%s_%d", outpath, p), maxfile)
					for ; b != e; b++ {
						_, tile_data_id := unpair(shallow[b])
						loc, ok := tile_cache[tile_data_id]
						if ok {
							_ = loc
						} else {
							getTileData.QueryRow(tile_data_id).Scan(&tile_data)
							wr.Write(tile_data)
						}
					}
				}
			}
			wg.Done()
		}()
	}

	wg.Wait()
	return nil

}

/*
type MbtileSet struct {
	p         Pyramid
	db        *sql.DB
	getTileId *sql.Stmt
	getTile   *sql.Stmt

	// maps a pyramid index to a value in the Repeats dictionary.
	repeated         map[int]int
	repeated_tile_id []int
}

var _ BlobArray = (*MbtileSet)(nil)

func (b *MbtileSet) Len() int {
	return b.p.Len
}

const s1 = "select tile_id,count(*) from shallow_tiles group by 1 having count(*)>1 order by 1"

func NewMbtileSet(path string) (*MbtileSet, error) {
	// open sqlite and find the repeated tile ids.
	db, e := sql.Open("sqlite", path)
	if e != nil {
		return nil, e
	}
	s1, e := db.Prepare(s1)
	if e != nil {
		return nil, e
	}
	s2, e := db.Prepare(" ")
	if e != nil {
		return nil, e
	}
	// find the repeats an intialize those directories.

	return &MbtileSet{
		p:         Pyramid{},
		db:        db,
		getTileId: s1,
		getTile:   s2,
	}, nil
}

// start with the most used tiles

// getChunks implements Bucketable
// repeats at beginning.
func (m *MbtileSet) Read(start int, end int) ([][]byte, []int, error) {
	var b []byte
	r := make([][]byte, 0, end-start)
	rs := []int{}

	for ; start < len(m.repeated_tile_id); start++ {
		m.getTile.QueryRow(m.repeated_tile_id[start]).Scan(&b)
		// we might need to copy the block? does scan overwrite?
		r = r.append(b)
	}

	start -= len(m.Repeats)
	for ; start != end; start++ {
		x, y, z, err := m.p.Xyz(start)
		if err != nil {
			return nil, nil, err
		}
		var tileid int
		err = m.getTileId.QueryRow(x, y, z).Scan(&tileid)
		if err != nil {
			return nil, nil, err
		}

		sym, ok := m.repeated[tileid]
		if !ok {
			m.getTile.QueryRow(tileid).Scan(&b)
			r = append(r, b)
			rs = append(rs, 0)
		} else {
			rs = append(rs, sym)
		}
	}
	return r, rs, nil
}
*/



// Patched binary interpolative coding
type Pbic struct {
	key []uint32
	// predicts
	value  []uint64
	length []uint64
}

type Patch struct {
	offset uint64
	length uint32
	pos    uint32
}
type PbicEncoder struct {
	patch []Patch
}

// destroys input.
func (e *PbicEncoder) Encode(pos uint64, key, size []uint32, offset []uint64, w io.Writer) int {
	// is this the +1 idea better than just creating a bitset? bitsets are easy to compress. we still need to make offset move forward though or we can't use bic. Could we store the varint directly in the blob? Could create an extra io, but also could save io by keeping more good data in the interior node.

	// go through and remove the patches and adjust the offsets
	for i := range key {
		if offset[i] != pos {
			e.patch = append(e.patch, Patch{offset[i], size[i], uint32(len(e.patch))})
			offset[i] = pos + uint64(i)
		} else {
			pos += uint64(size[i])
			offset[i] += uint64(i)
		}
	}
	// we sort and uniquify the patches into a dictionary, then varint code the
	// entry in this dictionary. we need to keep their references

	sort.Slice(e.patch, func(a, b int) bool {
		return false
	})
	return 0
}
func PbicDecode(length int, r io.Reader) {

}

	// sort

	// a run of values stored sequentially in the log
	dense := []uint64{}

	// skey will hold all the non-sequential keys
	// the first key is non-sequential
	// then
	skey := []uint32{}
	srun := []uint32{}
	soffset := []uint64{}
	ssize := []uint32{}

	state := 1
	for i := 1; i < len(x.key); i++ {
		seq := x.key[i-1]+1 == x.key[i]
		repeat := x.offset[i-1] == x.offset[i]
		seqvalue := x.offset[i-1]+uint64(x.size[i-1]) == x.offset[i]

		if seq && seqvalue {
			dense = append(dense, x.offset[i])
			srun[len(srun)-1]++
		} else if false {

		} else if seq && repeat {
			srun[len(srun)-1]++
		} else {
			srun = append(srun, 1)
			skey = append(skey, x.key[i])
		}
	}
	//v1 := compress32(x.key)

		if offset == x.pos {
		// sequential reference
		x.pos += uint64(size) + 1
	} else {
		// random reference
		x.pos++
		x.patchOffset = append(x.patchOffset)
		x.patchSize = append(x.patchSize)
	}
func PartitionRange(from, to, partitions int) []int {
	ln := to - from
	splits := make([]int, partitions+1)
	splits[partitions] = ln
	delta := float64(ln) / float64(partitions)
	o := delta
	for i := 1; i < partitions; i++ {
		splits[i] = int(math.Ceil(o))
		o += delta
	}
	return splits
}

func Unpair(x uint64) (uint32, uint32) {
	return uint32(x >> 32), uint32(x & ((1 << 32) - 1))
}

// Why Partitions?
// 1. Average of one file to read a z,x,y. Two reads with ranges; one streaming for header
// 2. Some parallelism
// 3. Split into smallish files rather than pack is more cdn friendly.

type Mbtiles struct {
	path string
	pyr  *Pyramid
	// pyr -> tile_id
	// we can keep a cache of tile_ids and only write them once.
	shallow     []uint64
	highPyramid []uint64
	fileStart   []uint64
}

func NewMbtiles(inpath string) (*Mbtiles, error) {
	return &Mbtiles{
		path: inpath,
	}, nil
}
